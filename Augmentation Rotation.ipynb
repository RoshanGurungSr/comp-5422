{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c9986a-cc6d-4efc-90fd-96cff6e800d8",
   "metadata": {},
   "source": [
    "#### **Data Augmentation using 90 dgree rotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828411eb-7f37-4a67-9201-f06029cba60e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d07d29e3-5a83-4996-aa49-e0d421f4a5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average image file per object = 1350\n",
    "def augument_horizontal_flip(data_dir, label, out_dir, num_transformation=1):\n",
    "    data_fp = os.path.join(data_dir, label)\n",
    "    out_fp = os.path.join(out_dir, label)\n",
    "    \n",
    "    aug_pipeline = Augmentor.Pipeline(source_directory=data_fp, output_directory=out_fp)\n",
    "    aug_pipeline.rotate_random_90(probability=0.5)\n",
    "    num_of_samples = int(2)\n",
    "    \n",
    "    aug_pipeline.sample(num_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433007e8-045f-4d13-850b-da74c154d43e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_names = ['cassette_player', 'chain_saw', 'church', 'english_springer', 'french_horn', 'garbage_truck', 'gas_pump', 'golf_ball', 'parachute', 'tench']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8566ab-f2ba-4c5e-8828-28ce2b47abad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"dataset2/\"\n",
    "out_dir = os.path.join(os.getcwd(), \"dataset_90_rotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f29a68-f5c3-458e-9028-d8f63a468841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/cassette_player."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=160x160 at 0x7FA5FB48C760>: 100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 323.86 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/chain_saw."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=160x314 at 0x7FA5FB48D1B0>: 100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 244.64 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/church."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=160x208 at 0x7FA5FB48C0D0>: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 262.94 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/english_springer."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=239x160 at 0x7FA5FB48CD30>: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 245.04 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/french_horn."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=240x160 at 0x7FA5FB48D090>: 100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 246.35 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/garbage_truck."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=201x160 at 0x7FA5FB48CBE0>: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 250.05 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/gas_pump."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=213x160 at 0x7FA5FB48CB80>: 100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 296.94 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/golf_ball."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=160x224 at 0x7FA5FB48D660>: 100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 325.82 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/parachute."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=171x160 at 0x7FA5FB48C400>: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 357.28 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 1 image(s) found.\n",
      "Output directory set to /home/sasmi/Desktop/Lakehead University/COMP-5422-FA Comp Vision & Image Analysis/Projects/comp-5422/dataset_90_rotation/tench."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=213x160 at 0x7FA5FB48D3C0>: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 242.10 Samples/s]\n"
     ]
    }
   ],
   "source": [
    "for folder_name in folder_names:\n",
    "    augument_horizontal_flip(data_dir=data_dir, label=folder_name, out_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b2bb5-52c0-4768-be11-2f9c556754dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "299b8ab7-74a1-4281-966f-0e94149dca2b",
   "metadata": {},
   "source": [
    "#### **Mapping the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28643448-ed38-457d-8986-4e882217029a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import walk\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3785ad-f71b-4155-8077-86906a38022d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_dir = os.path.abspath(os.getcwd())\n",
    "data_path = \"dataset_90_rotation2/\" # Change the mapping directory\n",
    "data_mapping = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(data_path):\n",
    "    if filenames:\n",
    "        label_name = dirpath.split(\"/\")[-1]\n",
    "        data_full_fp = [os.path.join(current_dir, dirpath, fname) for fname in filenames]\n",
    "        \n",
    "        for data_fp in data_full_fp:\n",
    "            data_mapping.append([data_fp, label_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78b64b5-c3e3-488b-9fe6-82154f545376",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/sasmi/Desktop/Lakehead University/COMP-5...</td>\n",
       "      <td>chain_saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/sasmi/Desktop/Lakehead University/COMP-5...</td>\n",
       "      <td>chain_saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/sasmi/Desktop/Lakehead University/COMP-5...</td>\n",
       "      <td>gas_pump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/sasmi/Desktop/Lakehead University/COMP-5...</td>\n",
       "      <td>gas_pump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/sasmi/Desktop/Lakehead University/COMP-5...</td>\n",
       "      <td>golf_ball</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename      label\n",
       "0  /home/sasmi/Desktop/Lakehead University/COMP-5...  chain_saw\n",
       "1  /home/sasmi/Desktop/Lakehead University/COMP-5...  chain_saw\n",
       "2  /home/sasmi/Desktop/Lakehead University/COMP-5...   gas_pump\n",
       "3  /home/sasmi/Desktop/Lakehead University/COMP-5...   gas_pump\n",
       "4  /home/sasmi/Desktop/Lakehead University/COMP-5...  golf_ball"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(data_mapping, columns=[\"filename\", \"label\"])\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baaf4675-9b9d-47db-9af5-322ad2bf652b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    'cassette_player': 0,\n",
    "    'chain_saw': 1,\n",
    "    'church': 2,\n",
    "    'english_springer': 3,\n",
    "    'french_horn': 4,\n",
    "    'garbage_truck': 5,\n",
    "    'gas_pump': 6,\n",
    "    'golf_ball': 7,\n",
    "    'parachute': 8,\n",
    "    'tench': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b41e5afd-1511-4d3e-b943-e97d3c5e5df7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The test_size = 2 should be greater or equal to the number of classes = 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(test\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/jarvis/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/jarvis/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2670\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2666\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2668\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2670\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2673\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2674\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2675\u001b[0m     )\n\u001b[1;32m   2676\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/jarvis/lib/python3.10/site-packages/sklearn/model_selection/_split.py:1746\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1746\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/anaconda3/envs/jarvis/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2160\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   2158\u001b[0m     )\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_test \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[0;32m-> 2160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe test_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_test, n_classes)\n\u001b[1;32m   2163\u001b[0m     )\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;66;03m# Find the sorted list of instances for each class:\u001b[39;00m\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;66;03m# (np.unique above performs a sort, so code is O(n logn) already)\u001b[39;00m\n\u001b[1;32m   2167\u001b[0m class_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m   2168\u001b[0m     np\u001b[38;5;241m.\u001b[39margsort(y_indices, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmergesort\u001b[39m\u001b[38;5;124m\"\u001b[39m), np\u001b[38;5;241m.\u001b[39mcumsum(class_counts)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   2169\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: The test_size = 2 should be greater or equal to the number of classes = 10"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data_df, \n",
    "                               test_size=0.1, \n",
    "                               stratify=data_df[\"label\"], \n",
    "                               random_state=42)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e7aa69-0a5b-4687-ac37-ff451a0d6053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.to_csv(os.path.join(data_path, \"train.csv\"), index=False)\n",
    "test.to_csv(os.path.join(data_path, \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6bb1c-c5ec-4219-a3d5-ed299f8a77f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11923824-5dc8-445d-9a1f-a77a7eddcb72",
   "metadata": {},
   "source": [
    "#### **Modelling Using the Augmentated Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50c330-c9c0-4b61-971b-45aa007d8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca994b-5fbc-4aac-aec2-e4949bb2a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, labels_map, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels['label'] = self.img_labels['label'].map(labels_map)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_labels.iloc[idx, 0]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        label = torch.tensor(int(self.img_labels.iloc[idx, 1]), dtype=torch.uint8)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06b498-6298-4705-9a68-a085767ad6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self.resnet18 = models.resnet18(pretrained=False)\n",
    "        self.resnet18.fc = nn.Linear(in_features=512, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.resnet18(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e56abb-0989-46fb-84a1-f7d00074de74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dir(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f65ed-142c-43ed-9317-b74ba0b307cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dump_model(training_id, model_dir, model):\n",
    "    model_name = f\"{training_id}_resnet18_od.pth\"\n",
    "    create_dir(dir_name=model_dir)\n",
    "        \n",
    "    model_dump_fp = os.path.join(model_dir, model_name)\n",
    "    torch.save(model.state_dict(), model_dump_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93090493-3afa-4a2a-8807-ae293dd587fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    'cassette_player': 0,\n",
    "    'chain_saw': 1,\n",
    "    'church': 2,\n",
    "    'english_springer': 3,\n",
    "    'french_horn': 4,\n",
    "    'garbage_truck': 5,\n",
    "    'gas_pump': 6,\n",
    "    'golf_ball': 7,\n",
    "    'parachute': 8,\n",
    "    'tench': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252b98c-825f-470a-a17a-a60b40d86f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_annotation_fp = \"dataset_90_rotation/train.csv\"\n",
    "test_annotation_fp = \"dataset_90_rotation/test.csv\"\n",
    "\n",
    "model_dir = \"./models\"\n",
    "training_id = f\"horizontal_flip_{datetime.utcnow().timestamp()}\"\n",
    "\n",
    "log_dir = os.path.join(\"./log_dir\", training_id)\n",
    "create_dir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0d86e-6563-468a-b36f-cdb63acae643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-18 expects input images of size (224, 224)\n",
    "feature_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = CustomImageDataset(annotations_file=train_annotation_fp, \n",
    "                                   labels_map=labels_map,\n",
    "                                   transform=feature_transform,\n",
    "                                  )\n",
    "test_dataset = CustomImageDataset(annotations_file=test_annotation_fp, \n",
    "                                  labels_map=labels_map,\n",
    "                                  transform=feature_transform,\n",
    "                                 )\n",
    "\n",
    "# Define the DataLoader\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb13a73-1b59-448e-9913-cba1313bb3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbbbdc1-d864-4d92-8d9e-1e55a92d1ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting some image samples from the dataloaders\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(20):\n",
    "    train_features, train_labels = next(iter(train_dataloader))\n",
    "    plt.subplot(5,4,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_features[0].numpy().transpose((1, 2, 0)))\n",
    "    plt.xlabel(train_labels[0].tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7ef48-7973-442c-b774-52085d74e091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_class = 10\n",
    "learning_rate = 1e-03\n",
    "num_epoch = 20\n",
    "save_model = True\n",
    "\n",
    "model = ResNetModel(num_classes=num_class)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa7639-0b72-44d7-8038-99beee7951aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "epoch_ls, total_loss, total_accuracy = [], [], []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    loss_train, acc_train = 0, 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # calculate the loss and accuracy\n",
    "        loss_train += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        acc_train += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = loss_train / len(train_dataloader)\n",
    "    epoch_accuracy = acc_train / total_samples\n",
    "    \n",
    "    epoch_ls.append(epoch)\n",
    "    total_loss.append(epoch_loss)\n",
    "    total_accuracy.append(epoch_accuracy)\n",
    "    \n",
    "    print(f\"Current epoch: {epoch} | Training Accuracy: {epoch_accuracy} | Training Loss: {epoch_loss}\")\n",
    "    \n",
    "if save_model:\n",
    "    dump_model(training_id, model_dir, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95185b1-e710-4fad-bf6d-bf0110d70d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "test_acc = 100 * correct // total\n",
    "print(f'Test Accuracy: {test_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8bf01c-f614-4ff5-a727-5473737fea95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_metrics_df = pd.DataFrame(zip(epoch_ls, total_loss, total_accuracy), columns=[\"epoch\", \"loss\", \"accuracy\"])\n",
    "train_metrics_df.to_csv(f\"{os.path.join(log_dir, 'training_metrics.csv')}\", index=False)\n",
    "train_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6636e0f-93c4-4918-831c-fd1da20d7350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_acc = 100 * correct // total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a54ab-7ef1-4cff-95ae-ab943484e914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_metrics_df = pd.DataFrame([test_acc], columns=[\"accuracy\"])\n",
    "test_metrics_df.to_csv(f\"{os.path.join(log_dir, 'test_metrics.csv')}\", index=False)\n",
    "test_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e63c4-140f-4cfa-8986-5cafccf17d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=pd.melt(train_metrics_df, [\"epoch\"]), x=\"epoch\", y=\"value\", hue=\"variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a178281-dcf2-42bc-89b1-fa4c4aa86a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c18ff-eb36-4e03-9f74-977d891bc221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "735216a2-1c22-434c-92fd-c7e46c68713a",
   "metadata": {},
   "source": [
    "### Deprecated Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8099457-f2df-4a24-a3c7-bddc8c7c4872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img = train_features[0].numpy().transpose((1, 2, 0))\n",
    "# label = train_labels[0]\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jarvis",
   "language": "python",
   "name": "jarvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
